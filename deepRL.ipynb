{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL basics\n",
    "** Agent (A) ** is acting in an ** environment (\\R) **\n",
    "Agent has:\n",
    "- Policy (behaviour function), PI\n",
    "- Value function\n",
    "- Model (representation of environment\n",
    "- State (s)\n",
    "- Agent can take an action (a)\n",
    "\n",
    "** State ** is the summary of experience,\n",
    "where experience is a sequence of observations, actions, rewards\n",
    "$$s_t = f(... o_t-1, r_t-1, a_t-1, o_t, r_t, a_t)$$\n",
    "or in a fully observed environment $s_t = f(o_t)$\n",
    "\n",
    "** Policy ** is a map from state to action.\n",
    "\n",
    "Either deterministically: $a = \\pi(s)$\n",
    "\n",
    "Or stochastically: $\\pi(a|s) = P[a|s] (?)$\n",
    "\n",
    "** Value function ** is a prediction of future reward.\n",
    "E.g. Q-value gives expected total reward\n",
    "$$Q^\\pi(s, a) = E[\\sum_n(\\gamma^n * r_t+n | s, a]$$\n",
    "which decomposes into a Bellman equation (WTF?)\n",
    "$$Q^\\pi(s, a) = E_s',a'[r + \\gamma*Q^pi(s',a') | s, a]$$\n",
    "\n",
    "$$Q^*(s,a) = Max_pi(Q^pi(s,a)) = Q^*(s,a)$$\n",
    "The optimal value function is the maximum achievable value,\n",
    "with wich we can act optimally:\n",
    "$$pi^*(s) = ArgMax_a Q^*(s,a)$$\n",
    "Optimal values decompose into a Bellman equation\n",
    "$$Q^*(s,a) = E_s'[r + lambda * max_{a'}Q^*(s',a') | s, a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to RL:\n",
    "- Value-based: estimates optimal value function, $Q^*(s,a)$\n",
    "- Policy-based: searches for policy achieving maximum future reward\n",
    "- Model-based: plans using learnt model\n",
    "ANN can represent the value function, the policy and the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value-based RL\n",
    "### Q-Networks\n",
    "Value function is a Q-Network with weights W: $Q(s,a,w) ~ Q^*(s,a)$\n",
    "Minimize MSE on\n",
    "$$L = (r + \\gamma Max_a Q(s',a',\\mathbf{w}) - Q(s,a,\\mathbf{w}))^2$$\n",
    "This converges to Q* if lookop table representation is used\n",
    "\n",
    "ANN diverges! Because samples correlate and targets are non-stationary (WTF?)\n",
    "\n",
    "** Removing correlations **\n",
    "\n",
    "Build dataset from agent's own experience (?):\n",
    "\n",
    "$s_t, a_t, r_{t+1}, s_{t+1} -> s, a, r, s'$, for every $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based RL\n",
    "### Deep Policy Networks\n",
    "$a = \\pi(a|s, \\mathbf{u})$ or $a = \\pi(s,\\mathbf{u})$\n",
    "\n",
    "$L(\\mathbf{u}) = \\mathbb{E}[\\sum_n \\gamma^n * r_n | \\pi(.,\\mathbf{u})]$, Total Discounted Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}